\documentclass{acm_proc_article-sp}
\usepackage{graphicx}
\usepackage{amsmath}
\def\url#1{{\tt#1}}

\begin{document}

\title{Advanced forensic Ext4 inode carving}

\numberofauthors{3} 
\author{
\alignauthor Jakub Marou\v sek \\
	\affaddr{Univerza v Ljubljani}\\
    \affaddr{Fakulteta za ra\v cunalništvo in informatiko}\\
    \affaddr{1000, Ljubljana}\\
    \email{jakub.marousek@gmail.com}%
\alignauthor Jan Ivanjko \\
	\affaddr{Univerza v Ljubljani}\\
    \affaddr{Fakulteta za ra\v cunalništvo in informatiko}\\
    \affaddr{1000, Ljubljana}\\
	\email{ji4988@student.uni-lj.si}%
\alignauthor Jernej Katanec \\
	\affaddr{Univerza v Ljubljani}\\
    \affaddr{Fakulteta za ra\v cunalništvo in informatiko}\\
    \affaddr{1000, Ljubljana}\\
	\email{jk6071@student.uni-lj.si}%
}

\maketitle
\begin{abstract}
Place an abstract here.
\end{abstract}

\section{Introduction}

\cite{braams:babel}

\section{The {\secit Body} of The Paper}

\subsection{Introduction to ext4}

The ext4 filesystem is the next generation in the family of ext filesystems, used typically in Linux environments. In the very beginning, the Linux kernel used Minix filesystem, due to the fact that Linux itself originated from Minix. This filesystem, however, contained serious limitations, the maximum file name size and the maximum filesystem size as the most severe examples. A group of kernel developers, with Theodore Ts'o being the most prominent one, introduced a couple of new filesystems \cite{ext2design}.

The first new filesystem, ext (``extended file system'', released in 1992) removed the Minix limitations, but some of the previous problems remained -- for example, free blocks were monitored over a linked list, which harmed performance. As a quick response, ext2 (``second extended file system'', 1993) was released. While the code of ext2 originated in that of ext, it brought many notable improvements.

\subsubsection{The ext2 filesystem}

The filesystem supports standard Unix file types: regular files, directories, device-special files and links. The maximum size of a file is 2 GB, the maximum size of the whole filesystem is 4 TB.
As the other Unix filesystems, it also uses inodes for storing file metadata. An {\it inode} is a structure containing a description of a file: file type, access rights, owners, timestamps, size, pointers to data blocks.

A directory is only a special kind of file, with a list of files and their corresponding inodes. A link can be either hard or symbolic. A hard link is basically a file pointing to the same inode as the original file, not distinguishable from the previous file. To remove a hard-linked file, all the references to the file must be removed. A symbolic link contains a text specifying a path to the file. Unlike the symlink, creating a hard link has limitations: it is possible to hard-link only a file from the same partition (due to the nature of the link) and hard-linking directories is not allowed (to prevent infinite subdirectory loops). A device-special file is only a pointer to the device driver.

While these features are common for all Unix filesystems, now we shall list some abilities specifical for ext2. Using the file attributes, the kernel behavior of creating new files in a directory can be modified, specifically the new file user id and group id. The logical block sizes can be specified when the filesystem is created -- larger block sizes lead to efficient behavior, but also wastes more space.

The symbolic links of ext2 can use more efficient way of storing the path inside the inode of the symlink. The overall status of the filesystem is saved in the superblock. During the mounting of the filesystem, if it is detected that it was not unmounted properly, the filesystem is automatically checked. After a certain number of mounts or when a certain time period has passed, the filesystem checking is enforced.

It is also possible to create {\it immutable} files, which either cannot be written into or deleted, or they can be opened for writing, but the text is appended to the end of the file.

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth]{images/ext2structure.jpg}
	\caption{The structure of an ext2 filesystem \cite{ext2structure}}
\end{figure}

{\bf The filesystem structure} is as follows. There is a boot sector in the beginning of the space, which is designated for bootloaders code. The rest of the drive is split into {\it block groups}. Each block groups contains inodes (in an inode table) and data blocks. An inode bitmap and a block bitmap marks which inode/block is used and which is not. 

The superblock and group descriptors contain information which are crucial for the filesystem functioning, and thus they are put in the beginning of each group as a backup. The superblock contains the basic information about the structures, like logical block sizes, the number of blocks and inodes, positions of data blocks and so on. An array of group descriptors contains pointers to the inode table and the block table for every present group.

As the main optimization, ext2 performs block readahead, that means, reads more continguous blocks at once. Block groups try to keep together inodes and blocks of a file, which reduces disk seeks. Also when a file is being written, ext2 preallocates up to 8 blocks.

\subsubsection{The ext3 file system}

After the initial period of bugs elimination, the ext2 file system has become a safe, stable choice for a Linux-based operating system. One of the main drawbacks of using ext2 is the lack of jounrnaling, which slows down the file system check and makes files prone to corruption.

First suggestions for the journaling of ext file system family were put down by Stephen C.\ Tweedie \cite{extjournal}. He identifies several aspects of file system reliability and puts forward {\it preservation} (``stable'' data on the disk should never be damaged), {\it predictability} (the failure modes of the file system should be predictable and deterministic) and {\it atomicity} (every operation is either fully performed or fully undone). The ext2 provided only the preservation aspect, whilst the failure states (could occur because of an unexpected reboot, for example) were not properly defined and might have prevented the operations from fully performing.

The Tweedie's article \cite{extjournal} compares approaches of reaching the desired aspects. The easiest way is to wait for a disk write to complete before the next one is submitted, which breaks the performance. Another idea lies in preserving multiple write buffers, ordering them accordingly and write these separately. However, one can easily get into a situation when the buffer dependendence is cyclic (moving a file from directory $A$ to directory $B$ and, in the same time, moving another file from $B$ to $A$). While there exists a mechanism to solve it, all the presented approaches require the disk to be completely rescanned for errors in case of a system failure. (Recall that file system check slowness was one of main objections against ext2.)

Instead, the author proposes to use {\it journaling}. To ensure operations to be atomic, a batch of data is written on the disk, but is not effective until a special {\it commit} block does not conclude the operation. Thus, a failure during performing the write can result only in two possibilities: either the commit block has been written on the disk, which means the transaction is complete, or it has not been written, in which case the transaction as a whole is undone. More practically: there is a dedicated place on the disk, called a {\it journal}, working as a cycling buffer. A transaction starts with a description block, continues with all the blocks that the transaction is modifying, and end with a commit block. Only after all these blocks have been successfully written to the journal, the ``real'' blocks modification is done. If a failure of the system occurs, the recovery program goes through the journal and performs all transactions which contain the commit block. If the block is not present, the transaction is discarded.

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth]{images/journal.pdf}
	\caption{The structure of ext3 journal \cite{takingadvantage}}
\end{figure}

The ext3 file system, released in 2001, i offers three different modes of journaling: \cite{takingadvantage}

\begin{itemize}
	\item {\it Journal} -- All data and matadata are logged. This approach minimizes the chance of losing data, but has the laregst performance penalty.
	\item {\it Ordered} -- Only metadata are logged, but file data are flushed on the disk before the change of metadata occurs, keeping the journal synchronized with data writes. This is the default mode.
	\item {\it Write back} -- Only metadata are logged, file data are flushed whenever possible. This mode is the fastest, but also the most dangerous.
\end{itemize}

Notice that journaling cannot be turned off. Compared to ext2, this is the only different feature. Thanks to this, it is possible to convert an ext2 partition to ext3 with spawning a single command.
For the period of ext3 development, a principle of both backward and forward compatibility with ext2 was closely followed by its authors. Therefore, an ext3 partition can be mounted with ext2 driver and works the same with the exception of journaling. Turning off a journal causes an ext3 partition to become ext2 and vice versa.

\subsubsection{The ext4 file system}

Although ext3 was a step forward, it still lacked some state-of-the-art features, which could be found in other Unix filesystems. Between 2003 and 2006, numberous patches for the ext3 were proposed to be added to the Linux kernel, but they were not accepted because the Linux community was afraid of stability of their mostly-used-file system at that time. As a result, Theodore Ts'o introducted plans for creating a successor of ext3, which would partially break the forward compatibility, but still allowed an easy update from the older versions \cite{newext4}. In 2008, the code was marked as stable.

A large amount work was done in terms of scalability. The maximum size of the file system was raised to 1 EB, the maximum size of a file is raised to 2 TB. The maximum number of files is increased too.

A compatibility-breaking change of file block locations is brought up. Instead of storing a list of data blocks (which is advantageous for small files but unpractical for large ones), {\it extents} are introduced. An extent is a range of data blocks, where all blocks in the range belong to the file. Four extents can be directly stored in the inode of the file. For large and fragmented files, a tree of pointers is maintained, with extents being in the leaves.

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth]{images/extents.pdf}
	\caption{Extent tree layout \cite{newext4}}
\end{figure}

The transactions in the journal are checksummed in the commit block, making two consequences: the recovery is more reliable as the corrupted transaction blocks are detected and the transaction cancelled; also now the commit block can be written in the same time as the other blocks, which significantly improves the performance.

Contrary to ext4, persistent pre-allocation of file size is possible. The checking of the file system is 2 to 20 times 2 to 20 times faster, as the unallocated inodes are marked and are skipped during the rest of the procedure. Instead of allocating one block at a time, the allocation is {\it delayed} in ext4, meaning it is postponed in time and performed when the file is flushed. There is also a possibility of online defragmentation. Since the size of an inode is larger, nanosecond resolution timestamps are supported and the problem of year 2038 is deferred for 272 years.

Similarly as in the case of ext2 and ext3, migrating to ext4 is just a matter of turning on the new features. The legacy system of individual block mapping is preserved, but all new files will use extents. Downgrading from ext4 to ext3 by turning off the features is straightforward when extents are not used -- otherwise, temporary copy of the files is needed.

\subsubsection{Comparison of ext4 and other Unix file systems}

It is worth noting that Theodore Ts'o considered ext4 not to be a major advance and still using old technology. In the light of his words, I found potentially interesting to list other popular Unix filesystems and compare their functions.

{\it Btrfs} \cite{btrfs} is a file system brought to life by Oracle. The development began in 2007 and the software was proclaimed to be stable in 2014. The name comes from the B-tree as this is the main structure used within the file system. Notable features of Btrfs are copy-on-write (a copied and unmodified file references to the ``old data''), snapshots, CRC checksums of data, RAID or configurable compression. In order to prolong the lifespan, SSD drivers are handled specifically. In-place conversion from ext2/3/4 and ReiserFS is present.

{\it ReiserFS} was created by company Namesys with Hans Reiser as the chief developer. When the first version was released in 2001, it brought attention because of features which had been unavailable, like journaling, online filesystem resizing, packing files in a single disk block or B-tree usage. Although ReiserFS was popular in the past, it was overtaken by other file systems since then. The main reason may be the slow pace of development, especially after Hans Reiser has been imprisoned and convicted of murder.

{\it ZFS} \cite{zfs} was developed by Oracle, but after the end of support the project was handled to the open-source community \cite{openzfs}. It supports journaling, read-only snaphots of the files state, RAID or checksums of files. On Ubuntu, ZFS has been chosen as the default filesystem for containers.

\subsection{File carving}

File carving, or sometimes just "carving" is the process of extracting data collection from a larger data set. Digital investigation often include data carving techniques when unallocated file system space is analysed to extract files. These files are carved from the unallocated space using file type-specific header and footer values.

File carving is not the same as file recovery. File recovery techniques rely on the file system information that remains after the deletion of a file to recover those files. On the other hand file carving techniques are used to restore as much data or data fragments as possible, when the file system is corrupted or deleted. Carving or raw data recovery process does not rely on the file system structures. It searches block by block for data matching the specific file type header and footer values. 

In digital forensics carving is especially useful in criminal cases, becouse it can recover evidence. As long as data on a disk is not overwritten or wiped, it can be restored using file carving techniques. Sometimes even data from formatted drives can be restored if the conditions are right. The most common general techniques to carve files are:

\begin{itemize}
\item \textbf{File Structure Based carving}
This technique uses identifier string, header, footer and size information to assume internal layout of a file. Header is a unique identifier, its value identifies the type of a file. Its existence means we can identify the beginning of a file, while the existence of a footer shows the tail of a file.
The blocks between the header and the footer represent the targeted file. In some cases file format has no footer, therefore a maximum file size is used in the carving program.
\item \textbf{Content based carving}
Carving based on content structure (MBOX, HTML, XML) or linguistic analysis of the file's content. A semantic carver might conclude that some blocks of German in a middle of a long HTML file written in English is a fragment left from a previous allocated file, and not from the English HTML file. Similarly for other content characteristics like, character count, information entropy, white and black listing of data.
\end{itemize}
Carving can be classified as basic and advanced, with basic it is assumed that:
\begin{itemize} 
\item the beginning of file is not overwritten
\item the file is not fragmented
\item the file is not compressed
\end{itemize}
Advanced carving relies on internal file's structure and occurs even to fragmented files, where fragments can be:
\begin{itemize} 
\item not sequential
\item out of order
\item missing
\end{itemize}
Since basic carving does not consider the file's content the  attention has shifted to  advanced  carving methods. Header and footer are not enough to carve files because the file's content is not checked nor is sector within header/footer examined. Deeper knowledge of internal file's structure results in less false positives. 

Authors of the paper present an advanced method that uses patteren-based file carving . It searches for metadata structures of inodes to recover their content data. This approach avoids reading the superblock and group descriptor table since its goal is to recover files from reformatted or corrupted ext4 file systems. The presented method can be divided into five phases\cite{merola2008data}:
\begin{enumerate}
\item \textbf{Initialization}

The point of initialization  is to gather Ext4 parameters, they can be estimated from the file system size. The following parameters are of relevance:
\begin{itemize} 
\item Offset
\item File system size
\item Block size
\item Inode size
\item Inode ratio
\item Flex group size
\item 64 bit mode
\item Sparce superblock
\item Number of blocks per block group
\item Numbers of blocks and block groups in the file system
\item Number of inodes per block group
\item Space for growing group descriptor table
\end{itemize}
\item \textbf{Inode carving}

Not every 128 byte permutation forms a valid inode. For an inode to be correct, some of the values must be in certain relations. This fact is used while searching for potential inode candidates in a byte wise manner. The most significant 4 bits in the 2 byte structure of inode indicate its file type.

Search patterns can also be based on timestamps, such as time interval or its inner consistency. Creation date, modification date, deletion date timestamp consistency can be verified if the following conditions are true:
\begin{itemize}
\item Modification date\textless creation date
\item Deletion date=0 and (deletion date\textgreater modification date and deletion date\textgreater creation date
\item Modification, creation time and delete time must be valid
\end{itemize}
Extent header field must contain the statisticaly defined magic number 0xf30a. Other inode attributes can be used for search patterns. All found addresses of potential inodes are sorted by file type (regular files and directories) and used for recovery.
\item \textbf{Directory tree}

This phase tries to identify potential directory inodes. Directory entries are searched linearly where directories not strating with '.' or '..' are discarded. The file name and inode number are saved along with its parent inode number, therefore a logical tree forming the complete file path can be deducted.

The module must map physical inode addresses to inode numbers. The beggining of the inode table can be computed by equation:

\[s=(bg_a * n_{BG} +o_s + o_i +o_r)*b  \]
The mapping of an address to its inode number is computed by:

\[o_s=min \{1024, 1+ \left\lceil \frac{d*n_g+1024}{b} \right\rceil \} \]
\[ o_s=2*n_f\]
\begin{equation}
  X=
  \begin{cases}
    0, & \text{if}\ b=1024 \\
    1, & \text{otherwise}
  \end{cases}
\end{equation}
\[bg_a=\left\lfloor \frac{a}{b*n_{BG}} \right\rfloor \]

\[f(a)=(\frac{a-s}{i} +n_{i,BG} *bg_a +1) \]

where:
\begin{itemize}
\item b block size

\item $bg_a$ is block group of address a

\item $n_{BG}$ is number of inodes per block group

\item $o_s$ combined size of superblock, the group descriptor table and growth space

\item $o_i$=2*$n_f$, offset where $n_f$ is flex group size

\item $o_r$ first 1024 bytes of the file system are reserved independently of block size. This offset is in case the block size is 1024 and the reserved space takes whole block, shifting all addresses by one block 

\item i size of inode 
\end{itemize}

The mapping from physical addresses to inode numbers can be done using these formulas. By using information from directory trees, file names can be associated to inodes. In case of a irreparable directory, its children's file paths can not be reconstructed therefore their content is saved in files named after their inode address. All recovered files are named after their inode address and saved into a flat hierarchy.



\item \textbf{Regular files}

Having reconstructed file path or not the content of a regular file is now recovered. Ext4 file system stores file content scattered across the volume in a way managed by the extents. Since the file system journal can contain copies of the existing inodes, duplicates with different physical addresses can be found. In order for two inodes to be the same, the file size and the extent structures are compared, due to the lack of inode numbers.
\item \textbf{Files without content}

This is another optional phase and builds on the results of the directory tree and content data phase. The content of files found in the directory tree phase but not in the inode carving phase, cannot be recovered, but their filepath can. Therefore files are created empty with their original filename and file path.
\end{enumerate}

\subsection{Related works}

\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{paper}

\balancecolumns
\end{document}
